(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[931],{782:function(e,n,s){Promise.resolve().then(s.bind(s,8351))},8351:function(e,n,s){"use strict";s.r(n),s.d(n,{default:function(){return N}});var r=s(7437),t=s(2265),o=s(1907),a=s(1246);s(825),s(9714),s(507);var i=s(6044),l=s(6483);s(366);var c=function(){let e=new l.vd({});async function n(){try{return(await e.request("GET /repos/ivy-llc/ivy",{owner:"ivy-llc",repo:"ivy",headers:{"X-GitHub-Api-Version":"2022-11-28"}})).data}catch(e){return{stargazers_count:14012,forks_count:5868}}}let[s,o]=(0,t.useState)(14012),[a,c]=(0,t.useState)(5868),d=s.toLocaleString(),m=a.toLocaleString(),p=1402..toLocaleString();(0,t.useEffect)(()=>{n().then(e=>{o(e.stargazers_count),c(e.forks_count)})},[]);let{ref:f,inView:u}=(0,i.YD)({threshold:.5,triggerOnce:!0});return(0,r.jsxs)("div",{className:"github-stats",children:[(0,r.jsxs)("div",{className:"gh-stat",children:[(0,r.jsx)("img",{src:"/_next/static/media/star.34d3d938.png",className:"gh-star",alt:"Github Stars"}),(0,r.jsx)("div",{className:"stat-container",children:(0,r.jsxs)("p",{className:"gh-stat-title ".concat(u?"typed1":"hidden"),ref:f,children:[d," ",(0,r.jsx)("span",{className:"normal-weight",children:"Github Stars"})]})})]}),(0,r.jsxs)("div",{className:"gh-stat",children:[(0,r.jsx)("img",{src:"/_next/static/media/fork.01608773.png",className:"gh-fork",alt:"Github Forks"}),(0,r.jsx)("div",{className:"stat-container",children:(0,r.jsxs)("p",{className:"gh-stat-title ".concat(u?"typed2":"hidden"),children:[m," ",(0,r.jsx)("span",{className:"normal-weight",children:"Github Forks"})]})})]}),(0,r.jsxs)("div",{className:"gh-stat",children:[(0,r.jsx)("img",{src:"/_next/static/media/PR.db646a6a.svg",className:"gh-pr",alt:"Gitub Pull Requests"}),(0,r.jsx)("div",{className:"stat-container",children:(0,r.jsxs)("p",{className:"gh-stat-title ".concat(u?"typed3":"hidden"),children:[p," ",(0,r.jsx)("span",{className:"normal-weight",children:"Contributors"})]})})]})]})},d=s(9653);s(2758);var m=function(){return(0,r.jsxs)("div",{className:"main-title-div",children:[(0,r.jsx)("img",{src:d.default.src,className:"ivy-logo",alt:"ivy"}),(0,r.jsxs)("h2",{className:"main-title-heading",children:[(0,r.jsx)("span",{children:"Convert Machine Learning Code"}),(0,r.jsx)("span",{children:"Between Frameworks"})]}),(0,r.jsx)("video",{poster:"",id:"video1",width:"100%",height:"100%",autoPlay:!0,loop:!0,muted:!0,children:(0,r.jsx)("source",{src:"https://dl.dropboxusercontent.com/scl/fi/0k6jvh4iwetaoznc91eue/Ivy_hero_animation_18bg.mp4?rlkey=be89qp8xcycvr1ddvyk4un5yl&dl=0",type:"video/mp4"})}),(0,r.jsx)("video",{id:"video-mob",width:"100%",height:"100%",autoPlay:!0,loop:!0,muted:!0,playsInline:!0,children:(0,r.jsx)("source",{src:"https://dl.dropboxusercontent.com/scl/fi/dj6kikmu3lcriajuq18uq/Ivy_mobile_04bg.mp4?rlkey=m9mg2ff55zdj3g3x5qf21hgvi&dl=0",type:"video/mp4"})}),(0,r.jsx)("p",{className:"main-title-pip",children:"pip install ivy"})]})},p=s(1696);s(8749);var f="/_next/static/media/pytorch.79ef70e1.svg",u="/_next/static/media/tensorflow.b9efdda9.svg",h="/_next/static/media/jax.558668ce.svg",x="/_next/static/media/numpy.414a965a.svg",g=function(){let{ref:e,inView:n}=(0,i.YD)({threshold:.5,triggerOnce:!0}),s=n?"fade-in":"hidden";return(0,r.jsxs)("div",{className:"supported-frameworks",children:[(0,r.jsx)("div",{className:"framework-logo ".concat(s),ref:e,children:(0,r.jsx)("a",{href:"https://pytorch.org/",target:"_blank",rel:"noopener noreferrer",children:(0,r.jsx)("img",{src:f,alt:"PyTorch"})})}),(0,r.jsx)("div",{className:"framework-logo ".concat(s),ref:e,children:(0,r.jsx)("a",{href:"https://www.tensorflow.org/",target:"_blank",rel:"noopener noreferrer",children:(0,r.jsx)("img",{src:u,alt:"TensorFlow"})})}),(0,r.jsx)("div",{className:"framework-logo ".concat(s),ref:e,children:(0,r.jsx)("a",{href:"https://jax.readthedocs.io/en/latest/notebooks/quickstart.html",target:"_blank",rel:"noopener noreferrer",children:(0,r.jsx)("img",{src:h,alt:"JAX"})})}),(0,r.jsx)("div",{className:"framework-logo ".concat(s),ref:e,children:(0,r.jsx)("a",{href:"https://numpy.org/",target:"_blank",rel:"noopener noreferrer",children:(0,r.jsx)("img",{src:x,alt:"Numpy"})})})]})},y=s(1942);s(133);let _={fontSize:"1rem",lineHeight:"1.4"};var j=()=>(0,r.jsxs)("div",{className:"source-to-source-section",children:[(0,r.jsx)("h1",{children:"Source-to-Source Transpilation"}),(0,r.jsx)("p",{children:"The transpiler converts the original model source code to source code in the target framework. This makes it incredibly easy to modify the model once it's been transpiled to the new framework - just as simple as if it had been created using it originally."}),(0,r.jsxs)("div",{className:"code-block",children:[(0,r.jsx)("p",{className:"pt-code-block-title",children:"Source Code (PyTorch)"}),(0,r.jsx)(o.Z,{language:"python",style:a.Z,customStyle:_,children:"\nclass CNN(torch.nn.Module):\n    def __init__(self):\n        super(CNN, self).__init__()\n        self.conv = torch.nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n        self.relu = torch.nn.ReLU()\n        self.maxpool = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n        self.fc = torch.nn.Linear(4 * 28 * 28, 10)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n        x = x.view(x.shape[0], -1)\n        x = self.fc(x)\n        return x\n"})]}),(0,r.jsx)(y.sEQ,{className:"arrow-icon"}),(0,r.jsxs)("div",{className:"code-block",children:[(0,r.jsx)("p",{className:"transpile-code-block-title",children:"Source-to-Source Transpiler"}),(0,r.jsx)(o.Z,{language:"python",style:a.Z,customStyle:_,children:'\ntensorflow_CNN = ivy.source_to_source(CNN, source="torch", target="tensorflow")\n'})]}),(0,r.jsx)(y.sEQ,{className:"arrow-icon"}),(0,r.jsxs)("div",{className:"code-block",children:[(0,r.jsx)("p",{className:"tf-code-block-title",children:"Transpiled Code (TensorFlow)"}),(0,r.jsx)(o.Z,{language:"python",style:a.Z,customStyle:_,children:'\nclass tensorflow_CNN(tensorflow.keras.Model):\n    @tensorflow_store_config_info\n    def __init__(self):\n        self.super.__init__(\n            v=getattr(self, "_v", None),\n            buffers=getattr(self, "_buffers", None),\n            module_dict=getattr(self, "_module_dict", None),\n        )\n        with tensorflow.name_scope("tensorflow_CNN/conv"):\n            self.conv = tensorflow.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n        with tensorflow.name_scope("tensorflow_CNN/relu"):\n            self.relu = tensorflow.ReLU()\n        with tensorflow.name_scope("tensorflow_CNN/maxpool"):\n            self.maxpool = tensorflow.MaxPool2d(kernel_size=2, stride=2)\n        with tensorflow.name_scope("tensorflow_CNN/fc"):\n            self.fc = tensorflow.Linear(4 * 28 * 28, 10)\n\n    def call(self, x):\n        x = self.conv(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n        with tensorflow.name_scope("tensorflow_CNN/x"):\n            x = tensorflow.view(x, x.shape[0], -1)\n        x = self.fc(x)\n        return x\n'})]})]}),v={PyTorch:{TensorFlow:'\nimport ivy\nimport torch\nimport timm\nimport tensorflow as tf\n\n# Get a pretrained pytorch model\nmlp_encoder = timm.create_model("mixer_b16_224", pretrained=True, num_classes=0)\n\n# Transpile it into a keras.Model with the corresponding parameters\nnoise = torch.randn(1, 3, 224, 224)\nmlp_encoder = ivy.transpile(mlp_encoder, source="torch", to="tensorflow", args=(noise,))\n\n# Build a classifier using the transpiled encoder\nclass Classifier(tf.keras.Model):\n    def __init__(self):\n        super().__init__()\n        self.encoder = mlp_encoder\n        self.output_dense = tf.keras.layers.Dense(units=1000, activation="softmax")\n\n    def call(self, x):\n        x = self.encoder(x)\n        return self.output_dense(x)\n\n# Transform the classifier and use it as a standard keras.Model\nx = tf.random.normal(shape=(1, 3, 224, 224))\nmodel = Classifier()\nret = model(x)\n',NumPy:'\nimport ivy\nimport kornia\nimport requests\nimport numpy as np\nfrom PIL import Image\n\n# Transpile kornia from pytorch to numpy\nnp_kornia = ivy.transpile(kornia, source="torch", to="numpy")\n\n# get an image\nurl = "http://images.cocodataset.org/train2017/000000000034.jpg"\nraw_img = Image.open(requests.get(url, stream=True).raw)\n\n# convert it to the format expected by kornia\nimg = np.transpose(np.array(raw_img), (2, 0, 1))\nimg = np.expand_dims(img, 0) / 255\n\n# and use the transpiled version of any function from the library!\nout = np_kornia.enhance.sharpness(img, 5)\n',Jax:'\nimport ivy\nimport timm\nimport torch\nimport jax\nimport haiku as hk\n\n# Get a pretrained pytorch model\nmlp_encoder = timm.create_model("mixer_b16_224", pretrained=True, num_classes=0)\n\n# Transpile it into a hk.Module with the corresponding parameters\nnoise = torch.randn(1, 3, 224, 224)\nmlp_encoder = ivy.transpile(mlp_encoder, source="torch", to="haiku", args=(noise,))\n\n# Build a classifier using the transpiled encoder\nclass Classifier(hk.Module):\n    def __init__(self, num_classes=1000):\n        super().__init__()\n        self.encoder = mlp_encoder()\n        self.fc = hk.Linear(output_size=num_classes, with_bias=True)\n\n    def __call__(self, x):\n        x = self.encoder(x)\n        x = self.fc(x)\n        return x\n\ndef _forward_classifier(x):\n    module = Classifier()\n    return module(x)\n\n# Transform the classifier and use it as a standard hk.Module\nrng_key = jax.random.PRNGKey(42)\nx = jax.random.uniform(key=rng_key, shape=(1, 3, 224, 224), dtype=jax.numpy.float32)\nforward_classifier = hk.transform(_forward_classifier)\nparams = forward_classifier.init(rng=rng_key, x=x)\n\nret = forward_classifier.apply(params, None, x)\n'},TensorFlow:{PyTorch:'\nimport ivy\nimport torch\nimport tensorflow as tf\n\n# Get a pretrained keras model\neff_encoder = tf.keras.applications.efficientnet_v2.EfficientNetV2B0(\n    include_top=False, weights="imagenet", input_shape=(224, 224, 3)\n)\n\n# Transpile it into a torch.nn.Module with the corresponding parameters\nnoise = tf.random.normal(shape=(1, 224, 224, 3))\ntorch_eff_encoder = ivy.transpile(eff_encoder, source="tensorflow", to="torch", args=(noise,))\n\n# Build a classifier using the transpiled encoder\nclass Classifier(torch.nn.Module):\n    def __init__(self, num_classes=20):\n        super().__init__()\n        self.encoder = torch_eff_encoder\n        self.fc = torch.nn.Linear(1280, num_classes)\n\n    def forward(self, x):\n        x = self.encoder(x)\n        return self.fc(x)\n\n# Initialize a trainable, customizable, torch.nn.Module\nclassifier = Classifier()\nret = classifier(torch.rand((1, 244, 244, 3)))\n',NumPy:'\nimport ivy\nimport tensorflow as tf\nimport numpy as np\n\ndef loss(predictions, targets):\n    return tf.sqrt(tf.reduce_mean(tf.square(predictions - targets)))\n\n# transpile any function from tf to numpy\nnp_loss = ivy.transpile(loss, source="tensorflow", to="numpy")\n\n# get some arrays\np = np.array([3.0, 2.0, 1.0])\nt = np.array([0.0, 0.0, 0.0])\n\n# and use the transpiled version!\nout = np_loss(p, t)\n',Jax:'\nimport ivy\nimport jax\nimport os\nos.environ["SM_FRAMEWORK"] = "tf.keras"\nimport segmentation_models as sm\n\n# transpile sm from tensorflow to jax\njax_sm = ivy.transpile(sm, source="tensorflow", to="jax")\n\n# get some image-like arrays\nkey = jax.random.PRNGKey(23)\nkey1, key2 = jax.random.split(key)\noutput = jax.random.uniform(key1, (1, 3, 512, 512))\ntarget = jax.random.uniform(key2, (1, 3, 512, 512))\n\n# and use the transpiled version of any function from the library!\nout = jax_sm.metrics.iou_score(output, target)\n'},NumPy:{PyTorch:'\nimport ivy\nimport numpy as np\nimport torch\n\ndef loss(predictions, targets):\n    return np.sqrt(np.mean((predictions - targets) ** 2))\n\n# transpile any function from numpy to torch\ntorch_loss = ivy.transpile(loss, source="numpy", to="torch")\n\n# get some arrays\np = torch.tensor([3.0, 2.0, 1.0])\nt = torch.tensor([0.0, 0.0, 0.0])\n\n# and use the transpiled version!\nout = torch_loss(p, t)\n',TensorFlow:'\nimport ivy\nimport madmom\nimport tensorflow as tf\n\n# transpile madmom from numpy to tensorflow\ntf_madmom = ivy.transpile(madmom, source="numpy", to="tensorflow")\n\n# get some arrays\nfreqs = tf.range(20) * 10\n\n# and use the transpiled version of any function from the library!\nout = tf_madmom.audio.filters.hz2midi(freqs)\n',Jax:'\nimport ivy\nimport numpy as np\nimport jax\nimport jax.numpy as jnp\njax.config.update(\'jax_enable_x64\', True)\n\ndef loss(predictions, targets):\n    return np.sqrt(np.mean((predictions - targets) ** 2))\n\n# transpile any function from numpy to jax\njax_loss = ivy.transpile(loss, source="numpy", to="jax")\n\n# get some arrays\np = jnp.array([3.0, 2.0, 1.0])\nt = jnp.array([0.0, 0.0, 0.0])\n\n# and use the transpiled version!\nout = jax_loss(p, t)\n'},Jax:{PyTorch:'\nimport ivy\nimport rax\nimport torch\n\n# transpile rax from jax to torch\ntorch_rax = ivy.transpile(rax, source="jax", to="torch")\n\n# get some arrays\nscores = torch.tensor([2.2, 1.3, 5.4])\nlabels = torch.tensor([1.0, 0.0, 0.0])\n\n# and use the transpiled version of any function from the library!\nout = torch_rax.poly1_softmax_loss(scores, labels)\n',TensorFlow:'\nimport ivy\nimport jax.numpy as jnp\nimport tensorflow as tf\n\ndef loss(predictions, targets):\n    return jnp.sqrt(jnp.mean((predictions - targets) ** 2))\n\n# transpile any function from jax to tensorflow\ntf_loss = ivy.transpile(loss, source="jax", to="tensorflow")\n\n# get some arrays\np = tf.constant([3.0, 2.0, 1.0])\nt = tf.constant([0.0, 0.0, 0.0])\n\n# and use the transpiled version!\nout = tf_loss(p, t)\n',NumPy:'\nimport ivy\nimport jax.numpy as jnp\nimport numpy as np\n\ndef loss(predictions, targets):\n    return jnp.sqrt(jnp.mean((predictions - targets) ** 2))\n\n# transpile any function from jax to numpy\nnp_loss = ivy.transpile(loss, source="jax", to="numpy")\n\n# get some arrays\np = np.array([3.0, 2.0, 1.0])\nt = np.array([0.0, 0.0, 0.0])\n\n# and use the transpiled version!\nout = np_loss(p, t)\n'}};let w=["PyTorch","TensorFlow","NumPy","Jax"],k={PyTorch:f,TensorFlow:u,NumPy:x,Jax:h};function N(){var e;let[n,s]=(0,t.useState)("PyTorch"),[i,l]=(0,t.useState)("TensorFlow"),d=(null===(e=v[n])||void 0===e?void 0:e[i])||"Code example for transpiling from ".concat(n," to ").concat(i," is not available.");return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(p.Z,{}),(0,r.jsx)(m,{}),(0,r.jsxs)("div",{className:"section single-line-div",children:[(0,r.jsx)("h1",{className:"main-heading",children:"Single Line Conversion"}),(0,r.jsxs)("p",{className:"ivy-intro",children:[(0,r.jsx)("span",{className:"ivy-text",children:"Ivy"})," allows deep learning models to be transpiled between frameworks with just a single line of code."]}),(0,r.jsxs)("div",{className:"transpile-demo",children:[(0,r.jsxs)("div",{className:"framework-selectors",children:[(0,r.jsxs)("div",{children:[(0,r.jsx)("label",{htmlFor:"source-framework",children:"Source Framework:"}),(0,r.jsx)("select",{id:"source-framework",value:n,onChange:e=>{s(e.target.value),l(w.find(n=>n!==e.target.value)||"TensorFlow")},className:"framework-select",children:w.map(e=>(0,r.jsx)("option",{value:e,children:e},e))}),(0,r.jsx)("img",{src:k[n],alt:"".concat(n," logo")})]}),(0,r.jsxs)("div",{children:[(0,r.jsx)("label",{htmlFor:"target-framework",children:"Target Framework:"}),(0,r.jsx)("select",{id:"target-framework",value:i,onChange:e=>{l(e.target.value)},className:"framework-select",children:w.filter(e=>e!==n).map(e=>(0,r.jsx)("option",{value:e,children:e},e))}),(0,r.jsx)("img",{src:k[i],alt:"".concat(i," logo")})]})]}),(0,r.jsx)("div",{className:"code-block",children:(0,r.jsx)(o.Z,{language:"python",style:a.Z,customStyle:{fontSize:"1rem",lineHeight:"1.4"},children:d})})]})]}),(0,r.jsx)(j,{}),(0,r.jsxs)("div",{className:"section uniting-div",children:[(0,r.jsx)("h1",{className:"sub-heading",children:"Uniting The Most Popular Frameworks"}),(0,r.jsx)("p",{className:"ivy-intro",children:"The transpiler brings cross-compatibility to some of the most widely-used and powerful frameworks in the industry."}),(0,r.jsx)(g,{})]}),(0,r.jsxs)("div",{className:"section community-div",children:[(0,r.jsx)("h1",{className:"sub-heading",children:"Join Our Community"}),(0,r.jsxs)("p",{children:["Join our growing community on a mission to make conversions between frameworks simple and accessible to all!",(0,r.jsx)("br",{}),"We welcome open-source contributions to Ivy in the form of ",(0,r.jsx)("strong",{children:"Pull Requests"})," and ",(0,r.jsx)("strong",{children:"Issues"})," on our GitHub repository.",(0,r.jsx)("br",{}),(0,r.jsx)("br",{})]}),(0,r.jsxs)("p",{children:["The Ivy Discord server is the perfect place to ask questions, get ideas for how to contribute, and get help from fellow developers and the ",(0,r.jsx)("span",{className:"ivy-text",children:"Ivy"})," team. We're looking forward to working with you!",(0,r.jsx)("br",{})]}),(0,r.jsx)(c,{})]})]})}},1696:function(e,n,s){"use strict";var r=s(7437);s(2265);var t=s(7138);s(8522);var o=s(9653),a=s(1942);n.Z=function(){return(0,r.jsxs)("nav",{className:"navbar",children:[(0,r.jsx)("div",{className:"logo",children:(0,r.jsx)(t.default,{href:"/",className:"title-button",children:(0,r.jsx)("img",{src:o.default.src,alt:"ivy"})})}),(0,r.jsxs)("ul",{className:"page-links",children:[(0,r.jsx)("li",{children:(0,r.jsx)(t.default,{href:"/enterprise",children:"Enterprise"})}),(0,r.jsx)("li",{children:(0,r.jsx)("a",{href:"https://ivy.dev/docs/",children:"Documentation"})}),(0,r.jsx)("li",{children:(0,r.jsx)("a",{href:"https://github.com/ivy-llc/ivy",target:"_blank",rel:"noopener noreferrer",children:(0,r.jsx)(a.hJX,{className:"icon"})})}),(0,r.jsx)("li",{children:(0,r.jsx)("a",{href:"https://discord.gg/r5mcSAfp",target:"_blank",rel:"noopener noreferrer",children:(0,r.jsx)(a.j2d,{className:"icon"})})})]})]})}},825:function(){},366:function(){},507:function(){},2758:function(){},8522:function(){},8749:function(){},133:function(){},9714:function(){},9653:function(e,n,s){"use strict";s.r(n),n.default={src:"/_next/static/media/full_logo_light_long.d36922d2.svg",height:46,width:124,blurWidth:0,blurHeight:0}}},function(e){e.O(0,[677,422,761,699,231,642,971,23,744],function(){return e(e.s=782)}),_N_E=e.O()}]);